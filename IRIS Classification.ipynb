{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9b5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ab31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa = \"C:/Users/suhas/Downloads/Iris-20220815T170150Z-001/Iris/setosa\"\n",
    "versicolor = \"C:/Users/suhas/Downloads/Iris-20220815T170150Z-001/Iris/versicolor\"\n",
    "verginica=\"C:/Users/suhas/Downloads/Iris-20220815T170150Z-001/Iris/virginica\"\n",
    "IMG_SIZE = 70\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915e56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '6conv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f09cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 49217.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 33010.42it/s]\n"
     ]
    }
   ],
   "source": [
    "name=[]\n",
    "names=[setosa,versicolor,verginica]\n",
    "for i in names:\n",
    "    for img in tqdm(os.listdir(i)):\n",
    "        path = os.path.join(i, img)\n",
    "        if \"setosa\" in img:\n",
    "            name.append(img)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e234c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b0dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 92.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 171.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "names=[setosa,versicolor,verginica]\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "\n",
    "for i in names:\n",
    "    for img in tqdm(os.listdir(i)):\n",
    "        path = os.path.join(i, img)\n",
    "        if \"setosa\" in img:\n",
    "            y_train.append([1,0,0])\n",
    "        if \"versicolor\" in img:\n",
    "            y_train.append([0,1,0])\n",
    "        if \"virginica\" in img:\n",
    "            y_train.append([0,0,1])\n",
    "\n",
    "        # loading the image from the path and then converting them into\n",
    "        # grayscale for easier covnet prob\n",
    "        img =np.array( cv2.imread(path, cv2.IMREAD_GRAYSCALE))\n",
    "\n",
    "        # resizing the image for processing them in the covnet\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        x_train.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0e33e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b868167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "#for cnn we use conv2d , flatten\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f68bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n",
    "x_train=x_train.reshape(150,70,70,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf3106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnmodel=Sequential()\n",
    "cnnmodel.add(Conv2D(64,kernel_size=4,activation='relu',input_shape=(70,70,1)))\n",
    "cnnmodel.add(Conv2D(32,kernel_size=4,activation='relu'))\n",
    "cnnmodel.add(Flatten())\n",
    "cnnmodel.add(Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c820b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93f2f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 5s 610ms/step - loss: 276.9070 - accuracy: 0.3200 - val_loss: 41.3999 - val_accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 2s 480ms/step - loss: 14.6920 - accuracy: 0.4533 - val_loss: 0.5698 - val_accuracy: 0.7933\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 2s 490ms/step - loss: 0.6615 - accuracy: 0.8133 - val_loss: 0.7826 - val_accuracy: 0.5933\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 2s 489ms/step - loss: 0.7364 - accuracy: 0.6933 - val_loss: 0.4858 - val_accuracy: 0.9467\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 2s 488ms/step - loss: 0.3917 - accuracy: 0.9333 - val_loss: 0.1283 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ea0bcfd60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnmodel.fit(x_train,y_train,validation_data=(x_train,y_train),epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b37cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
